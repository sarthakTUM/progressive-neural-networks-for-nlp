{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root_path = '../../../'\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build vocabularies of words and tags from datasets\"\"\"\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "from src.ner.encoder import WordEncoder, CharEncoder, ClassEncoder\n",
    "from src.ner.data import SLIterator\n",
    "from src.ner import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_to_txt_file(vocab, txt_path):\n",
    "    \"\"\"Writes one token per line, 0-based line id corresponds to the id of the token.\n",
    "\n",
    "    Args:\n",
    "        vocab: (iterable object) yields token\n",
    "        txt_path: (stirng) path to vocab file\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for token in vocab:\n",
    "            f.write(token + '\\n')\n",
    "            \n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict to json file\n",
    "\n",
    "    Args:\n",
    "        d: (dict)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        d = {k: v for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)\n",
    "\n",
    "\n",
    "def update_vocab(txt_path, vocab):\n",
    "    \"\"\"Update word and tag vocabulary from dataset\n",
    "\n",
    "    Args:\n",
    "        txt_path: (string) path to file, one sentence per line\n",
    "        vocab: (dict or Counter) with update method\n",
    "\n",
    "    Returns:\n",
    "        dataset_size: (int) number of elements in the dataset\n",
    "    \"\"\"\n",
    "    with open(txt_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            vocab.update(line.strip().split(' '))\n",
    "\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "def get_stats(data_iterator, words, tags):\n",
    "    sentence_gen = data_iterator.get_next_X()\n",
    "    label_gen = data_iterator.get_next_Y()\n",
    "    num_sentences = 0\n",
    "    num_labels = 0\n",
    "    for sent, label in zip(sentence_gen, label_gen):\n",
    "        assert len(label) == 1\n",
    "        words.update(sent)\n",
    "        tags.update(label)\n",
    "        if sent:\n",
    "            num_sentences += 1\n",
    "        if label:\n",
    "            num_labels += 1\n",
    "    assert num_sentences == num_labels\n",
    "    return num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_count = 1\n",
    "min_count_tag=1\n",
    "data_folder = os.path.join(root_path, 'src/tc/data/sst_binary')\n",
    "feats_folder = os.path.join(data_folder, 'feats')\n",
    "json_folder = data_folder\n",
    "if not os.path.exists(feats_folder):\n",
    "    os.makedirs(feats_folder)\n",
    "if k_fold:\n",
    "    data_folder = os.path.join(data_folder, 'split_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train\n",
      "loading val\n",
      "loading test\n"
     ]
    }
   ],
   "source": [
    "print('loading train')\n",
    "train_data_iterator = SLIterator(os.path.join(data_folder, 'train'))\n",
    "print('loading val')\n",
    "val_data_iterator = SLIterator(os.path.join(data_folder, 'val'))\n",
    "print('loading test')\n",
    "test_data_iterator = SLIterator(os.path.join(data_folder, 'test'))\n",
    "data_iterators = [train_data_iterator, val_data_iterator, test_data_iterator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting stats\n"
     ]
    }
   ],
   "source": [
    "words = Counter()\n",
    "tags = Counter()\n",
    "print('getting stats')\n",
    "num_sentences_train = get_stats(train_data_iterator, words, tags)\n",
    "num_sentences_val = get_stats(val_data_iterator, words, tags)\n",
    "num_sentences_test = get_stats(test_data_iterator, words, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding\n",
      "creating and saving maps..\n",
      "creating map: WORD\n",
      "saving map: WORD\n",
      "creating map: CHAR\n",
      "Char distribution:  Counter({'e': 465215, 't': 366247, 'a': 336081, 'i': 320054, 'o': 292440, 'n': 284376, 's': 281161, 'r': 244672, 'l': 198539, 'h': 173988, 'd': 140714, 'c': 130265, 'u': 119907, 'm': 118677, 'f': 100059, 'g': 94585, 'y': 86157, 'p': 78367, 'b': 65565, 'w': 59734, 'v': 49595, ',': 37314, 'k': 33378, '-': 30484, '.': 30147, \"'\": 22831, 'x': 7893, 'j': 7403, 'q': 4462, 'z': 4343, '`': 3779, '0': 1381, '1': 1160, '9': 775, '2': 707, ':': 665, 'Ã': 612, '\\\\': 606, '©': 487, ';': 434, '/': 422, '8': 395, '5': 376, '?': 337, '3': 302, '!': 216, '4': 213, '7': 211, '*': 184, '6': 182, '&': 95, '$': 58, '¯': 20, '±': 20, '\\xa0': 17, '#': 17, 'Â': 16, '¨': 13, '¦': 13, '¢': 12, '+': 11, '³': 10, '¼': 9, '\\xad': 8, '¡': 8, '=': 7, '£': 3, '´': 3, '§': 3, '__PAD__': 1, '__UNK__': 1, '¶': 1, '%': 1, '»': 1})\n",
      "saving map: CHAR\n",
      "creating map: CLASS\n",
      "Class Distribution:  Counter({'1': 47222, '0': 39352})\n",
      "saving map: CLASS\n"
     ]
    }
   ],
   "source": [
    "# for all types of features for which vocab has to be created, load encoders\n",
    "print('encoding')\n",
    "from collections import OrderedDict\n",
    "data_encoders = OrderedDict()\n",
    "label_encoders = OrderedDict()\n",
    "data_encoders[WordEncoder.FEATURE_NAME] = WordEncoder(os.path.join(root_path, 'src/tc/embeddings/glove.6B.300d.txt'),\n",
    "                                                      dim=300,\n",
    "                                                      type='glove')\n",
    "data_encoders[CharEncoder.FEATURE_NAME] = CharEncoder()\n",
    "label_encoders[ClassEncoder.FEATURE_NAME] = ClassEncoder()\n",
    "\n",
    "print('creating and saving maps..')\n",
    "# create vocabs with iterators and encoders.\n",
    "for feature_name, encoder in data_encoders.items():\n",
    "    print('creating map: {}'.format(feature_name))\n",
    "    encoder.create_map(data_iterators)\n",
    "    # save maps in the data folder.\n",
    "    print('saving map: {}'.format(feature_name))\n",
    "    for map_name, map in encoder.maps.items():\n",
    "        utils.save_map(map, map_name, feats_folder)\n",
    "\n",
    "for feature_name, encoder in label_encoders.items():\n",
    "    print('creating map: {}'.format(feature_name))\n",
    "    encoder.create_map(data_iterators)\n",
    "    # save maps in the data folder.\n",
    "    print('saving map: {}'.format(feature_name))\n",
    "    for map_name, map in encoder.maps.items():\n",
    "        utils.save_map(map, map_name, feats_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vocabularies to file...\n",
      "- done.\n",
      "saving dataset stats\n",
      "Characteristics of the dataset:\n",
      "- train_size: 83881\n",
      "- dev_size: 872\n",
      "- test_size: 1821\n",
      "- vocab_size: 18844\n",
      "- number_of_tags: 2\n",
      "- special_tokens: {'pad_WORD': '__PAD__', 'unk_WORD': '__UNK__', 'pad_CHAR': '__PAD__', 'unk_CHAR': '__UNK__', 'pad_CLASS': None, 'unk_CLASS': None}\n",
      "- data_iterators: {'train': 'SLIterator', 'val': 'SLIterator', 'test': 'SLIterator'}\n"
     ]
    }
   ],
   "source": [
    "# Save vocabularies to file\n",
    "print(\"Saving vocabularies to file...\")\n",
    "save_vocab_to_txt_file(words, os.path.join(json_folder, 'words.txt'))\n",
    "save_vocab_to_txt_file(tags, os.path.join(json_folder, 'tags.txt'))\n",
    "print(\"- done.\")\n",
    "\n",
    "# Save datasets properties in json file\n",
    "print('saving dataset stats')\n",
    "encoder_params = dict()\n",
    "encoders = [data_encoders, label_encoders]\n",
    "for encoder in encoders:\n",
    "    for encoder_name, enc in encoder.items():\n",
    "        encoder_params['pad_'+encoder_name] = enc.PAD\n",
    "        encoder_params['unk_'+encoder_name] = enc.UNK\n",
    "\n",
    "sizes = {\n",
    "    'train_size': num_sentences_train,\n",
    "    'dev_size': num_sentences_val,\n",
    "    'test_size': num_sentences_test,\n",
    "    'vocab_size': len(words),\n",
    "    'number_of_tags': len(tags),\n",
    "    'special_tokens': encoder_params,\n",
    "    'data_iterators': {'train': train_data_iterator.__class__.__name__,\n",
    "                       'val': val_data_iterator.__class__.__name__,\n",
    "                       'test': test_data_iterator.__class__.__name__}\n",
    "}\n",
    "save_dict_to_json(sizes, os.path.join(json_folder, 'dataset_params.json'))\n",
    "\n",
    "# Logging sizes\n",
    "to_print = \"\\n\".join(\"- {}: {}\".format(k, v) for k, v in sizes.items())\n",
    "print(\"Characteristics of the dataset:\\n{}\".format(to_print))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
