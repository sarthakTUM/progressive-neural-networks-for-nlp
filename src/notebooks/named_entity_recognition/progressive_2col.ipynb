{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a network for Named Entity Recognition with architecture mentioned in [].\n",
    "\n",
    "How to run:\n",
    "1. Specify the data_dir. The directory should contain the train, val, and test folders, along with the 'feats' folder obtained through feat.ipynb.\n",
    "2. specify the model directory. The directory needs to be created manually. It should contain a params.json.\n",
    "3. The description of the progNet specific parameters is given below in the respective cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root_path = '../../../'\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm import trange\n",
    "from src.ner import utils\n",
    "from src.booster.progressive_data_loader import DataLoader\n",
    "from src.booster.progressive_evaluate import evaluate\n",
    "from src.booster.progressive_encoder import CharEncoder, EntityEncoder, WordEncoder\n",
    "from src.booster.progNN.net import LSTMCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root_path, 'src/ner/data/bc5cdr_iobes_id')\n",
    "model_dir = os.path.join(root_path, 'src/ner/experiments/test/test_prog2col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "freeze_prev: whether to freeze the previous column\n",
    "best_prev: whether to load the best checkpoint of the previous column, or a randomly initialized network. This is to see whether there is some benefit from the PNN structure, or just through more capacity.\n",
    "linear_adapter: keep it to True mostly. False places a non-linear adapter. Check ProgNet paper for more details.\n",
    "best_target: whether to fine-tune the target model in the PNN way. This will load the best.pth of the target model. \n",
    "Without this, the target model is loaded randomly. Best to keep it False.\n",
    "\n",
    "The parameters below are optimal.\n",
    "\"\"\"\n",
    "\n",
    "freeze_prev = True\n",
    "best_prev = True\n",
    "linear_adapter = True\n",
    "best_target = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pretrained_model_dir: model directory to be used for 1st column.\n",
    "target_mode_dir: model directory for the target column.\n",
    "\n",
    "Specify the below parameters.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pretrained_model_dir = os.path.join(root_path, 'src/ner/experiments/disease/st/st_ncbi')\n",
    "target_model_dir = os.path.join(root_path, 'src/ner/experiments/disease/st/st_bc5cdr_all') # this is important only if best_target = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set the device to train on\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the parameters from json file\n",
    "pretrained_network_params = os.path.join(pretrained_model_dir, 'params.json') # these should be loaded from the pretrained model\n",
    "assert os.path.isfile(pretrained_network_params), \"No json configuration file found at {}\".format(pretrained_network_params)\n",
    "new_network_params = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(new_network_params), \"No json configuration file found at {}\".format(new_network_params)\n",
    "\n",
    "pre_params = utils.Params(pretrained_network_params)\n",
    "new_params = utils.Params(new_network_params)\n",
    "\n",
    "# use GPU if available\n",
    "pre_params.cuda = torch.cuda.is_available()\n",
    "new_params.cuda = torch.cuda.is_available()\n",
    "\n",
    "# 3. Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if new_params.cuda: torch.cuda.manual_seed(230)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "creating and loading data loaders\n",
      "freeze_prev: True\n",
      "best_prev: True\n",
      "pretrained model: ../../../src/ner/experiments/disease/st/st_ncbi\n",
      "new model : ../../../src/ner/experiments/test/test_prog2col\n",
      "new data: ../../../src/ner/data/bc5cdr_iobes_id\n"
     ]
    }
   ],
   "source": [
    "# 5. Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "# 5.1 specify features\n",
    "from collections import OrderedDict\n",
    "\n",
    "# 5.1.1 encoders for the new model\n",
    "logging.info('creating and loading data loaders')\n",
    "data_encoder = OrderedDict()\n",
    "data_encoder[CharEncoder.FEATURE_NAME] = CharEncoder(os.path.join(data_dir, 'feats'))\n",
    "data_encoder[WordEncoder.FEATURE_NAME] = WordEncoder(os.path.join(data_dir, 'feats'),\n",
    "                                                     dim=new_params.embedding_dim)\n",
    "label_encoder = OrderedDict()\n",
    "label_encoder[EntityEncoder.FEATURE_NAME] = EntityEncoder(os.path.join(data_dir, 'feats'))\n",
    "new_params.data_feats = []\n",
    "new_params.label_feats = []\n",
    "for feat in data_encoder:\n",
    "    new_params.data_feats.append(feat)\n",
    "for feat in label_encoder:\n",
    "    new_params.label_feats.append(feat)\n",
    "\n",
    "# 5.1.2 encoders for the previous model\n",
    "pretrained_data_encoder = utils.load_obj(os.path.join(pretrained_model_dir, 'data_encoder.pkl'))\n",
    "pretrained_label_encoder = utils.load_obj(os.path.join(pretrained_model_dir, 'label_encoder.pkl'))\n",
    "pre_params.data_feats = []\n",
    "pre_params.label_feats = []\n",
    "for feat in pretrained_data_encoder:\n",
    "    pre_params.data_feats.append(feat)\n",
    "for feat in label_encoder:\n",
    "    pre_params.label_feats.append(feat)\n",
    "\n",
    "# 5.2 load data\n",
    "# 5.2.1 data loader for the new model\n",
    "data_loader = DataLoader(new_params,\n",
    "                         data_dir,\n",
    "                         data_encoder,\n",
    "                         label_encoder,\n",
    "                         device=device)\n",
    "pretrained_data_loader = DataLoader(pre_params,\n",
    "                                    data_dir,# data_dir has to be the new data\n",
    "                                    pretrained_data_encoder,\n",
    "                                    device=device)\n",
    "\n",
    "# 6. Modeling\n",
    "logging.info('freeze_prev: {}'.format(str(freeze_prev)))\n",
    "logging.info('best_prev: {}'.format(str(best_prev)))\n",
    "logging.info('pretrained model: {}'.format(pretrained_model_dir))\n",
    "logging.info('new model : {}'.format(model_dir))\n",
    "logging.info('new data: {}'.format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading previous models and creating new model\n",
      "C:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "creating columns\n"
     ]
    }
   ],
   "source": [
    "# 6.1.1 Define the model architecture for 1st column\n",
    "logging.info('loading previous models and creating new model')\n",
    "c1_model = LSTMCRF(params=pre_params,\n",
    "                   char_vocab_length=pretrained_data_encoder[CharEncoder.FEATURE_NAME].vocab_length,\n",
    "                   num_tags=pretrained_label_encoder[EntityEncoder.FEATURE_NAME].num_tags,\n",
    "                   pretrained_word_vecs=torch.from_numpy(pretrained_data_encoder[WordEncoder.FEATURE_NAME].vectors),\n",
    "                   dropout=pre_params.dropout,\n",
    "                   decoder_type=pre_params.decoder,\n",
    "                   bidirectional=pre_params.rnn_bidirectional,\n",
    "                   freeze_embeddings=pre_params.freeze_wordembeddings).to(device).float()\n",
    "# 6.1.1.1. load the pre-trained model to fit the architecture\n",
    "if best_prev:\n",
    "    utils.load_checkpoint(os.path.join(pretrained_model_dir, 'best.pth'), c1_model)\n",
    "\n",
    "# 6.1.2 Define the model for 2nd column\n",
    "c2_model = LSTMCRF(params=new_params,\n",
    "                   char_vocab_length=data_encoder[CharEncoder.FEATURE_NAME].vocab_length,\n",
    "                   num_tags=label_encoder[EntityEncoder.FEATURE_NAME].num_tags,\n",
    "                   pretrained_word_vecs=torch.from_numpy(data_encoder[WordEncoder.FEATURE_NAME].vectors),\n",
    "                   dropout=new_params.dropout,\n",
    "                   decoder_type=new_params.decoder,\n",
    "                   bidirectional=new_params.rnn_bidirectional,\n",
    "                   freeze_embeddings=new_params.freeze_wordembeddings).to(device).float()\n",
    "\n",
    "if best_target:\n",
    "    utils.load_checkpoint(os.path.join(target_model_dir, 'best.pth'), c2_model)\n",
    "\n",
    "# 7. Convert to columns\n",
    "\"\"\"\n",
    "the columns must specify the name of the layers to be used for PNN transfer, in the order from input to output.\n",
    "The numbers in the bracket represent the input and output dimension of the layer respectively.\n",
    "\"\"\"\n",
    "logging.info('creating columns')\n",
    "from src.booster.progNN.column import Column\n",
    "column_1 = Column(model=c1_model,\n",
    "                  layers={'rnn_1': (130, 200),\n",
    "                          'rnn_2': (200, 200),\n",
    "                          'fc':  (200, pretrained_label_encoder[EntityEncoder.FEATURE_NAME].num_tags)},\n",
    "                  data_loader=pretrained_data_loader).to(device)\n",
    "column_2 = Column(model=c2_model,\n",
    "                  layers={'rnn_1': (130, 200),\n",
    "                          'rnn_2': (200, 200),\n",
    "                          'fc':  (200, label_encoder[EntityEncoder.FEATURE_NAME].num_tags)},\n",
    "                  data_loader=data_loader).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "linear adapter: True\n",
      "creating progressive net\n",
      "total params: 80911282\n",
      "total trainable params: 476880\n"
     ]
    }
   ],
   "source": [
    "# 8. create progressive net\n",
    "from src.booster.progNN.adapter import Adapter\n",
    "\n",
    "adapter = Adapter(prev_columns=[column_1], target_column=column_2, linear=linear_adapter).to(device)\n",
    "\n",
    "logging.info('creating progressive net')\n",
    "from src.booster.progNN.prognet import ProgressiveNet\n",
    "\n",
    "progNet = ProgressiveNet(prev_columns=[column_1],\n",
    "                         target_column=column_2,\n",
    "                         adapter=adapter,\n",
    "                         linear_adapter=linear_adapter,\n",
    "                         freeze_prev=freeze_prev).to(device).float()\n",
    "\n",
    "progNet_total_params = sum(p.numel() for p in progNet.parameters())\n",
    "progNet_total_trainable_params = sum(p.numel() for p in filter(lambda p: p.requires_grad,\n",
    "                                    progNet.parameters()))\n",
    "logging.info('total params: {}'.format(str(progNet_total_params)))\n",
    "logging.info('total trainable params: {}'.format(str(progNet_total_trainable_params)))\n",
    "\n",
    "# 1. define the metrics\n",
    "from src.ner.evaluation import accuracy_score, f1_score, precision_score, recall_score\n",
    "metrics = {'accuracy': accuracy_score,\n",
    "           'f1_score': f1_score,  # micro F1 score\n",
    "           'precision_score': precision_score,\n",
    "           'recall_score': recall_score}\n",
    "\n",
    "# 2. Define the optimizer\n",
    "optimizer = optim.SGD(params=filter(lambda p: p.requires_grad,\n",
    "                                    progNet.parameters()),\n",
    "                      lr=0.015,\n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progressive net\n",
      "Epoch 1/2\n",
      "Learning Rate : [0.015]\n",
      "100%|███████████████████████████████████████████████████████████████████| 456/456 [00:40<00:00, 11.33it/s, loss=69.278]\n",
      "- Train metrics: accuracy: 0.759 ; f1_score: 0.305 ; precision_score: 0.539 ; recall_score: 0.213 ; loss: 193.188\n",
      "- val metrics : accuracy: 0.931 ; f1_score: 0.637 ; precision_score: 0.573 ; recall_score: 0.718 ; loss: 38.027\n",
      "- test metrics : accuracy: 0.934 ; f1_score: 0.629 ; precision_score: 0.564 ; recall_score: 0.712 ; loss: 37.684\n",
      "- Found new best F1 score\n",
      "Epoch 2/2\n",
      "Learning Rate : [0.014285714285714284]\n",
      "100%|███████████████████████████████████████████████████████████████████| 456/456 [00:41<00:00, 11.08it/s, loss=49.868]\n",
      "- Train metrics: accuracy: 0.928 ; f1_score: 0.664 ; precision_score: 0.635 ; recall_score: 0.695 ; loss: 51.102\n",
      "- val metrics : accuracy: 0.933 ; f1_score: 0.694 ; precision_score: 0.700 ; recall_score: 0.688 ; loss: 34.859\n",
      "- test metrics : accuracy: 0.931 ; f1_score: 0.676 ; precision_score: 0.689 ; recall_score: 0.664 ; loss: 35.387\n",
      "- Found new best F1 score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Train ProgNet\n",
    "from src.booster.progressive_ner import train_and_evaluate\n",
    "logging.info('Training progressive net')\n",
    "train_and_evaluate(progNet,\n",
    "                   progNet.target_column.data_loader,\n",
    "                   progNet.target_column.data['train'],\n",
    "                   progNet.target_column.data['val'],\n",
    "                   progNet.target_column.data['test'],\n",
    "                   optimizer,\n",
    "                   metrics,\n",
    "                   new_params,\n",
    "                   model_dir,\n",
    "                   progNet.target_column.data_loader.data_encoder,\n",
    "                   progNet.target_column.data_loader.label_encoder,\n",
    "                   restore_file=None,\n",
    "                   save_model=False,\n",
    "                   eval=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
