{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a network for Named Entity Recognition with architecture mentioned in [].\n",
    "\n",
    "How to run:\n",
    "1. Specify the data_dir. The directory should contain the train, val, and test folders, along with the 'feats' folder obtained through feat.ipynb.\n",
    "2. specify the model directory. The directory needs to be created manually. It should contain a params.json.\n",
    "3. Specify the pretrained model directory. It should contain model.pth file.\n",
    "4. Specify the 'all_layer' parameter. True corresponds to all-layer fine-tuning, and False otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root_path = '../../../'\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm import trange\n",
    "from src.ner import utils\n",
    "from src.ner.model.data_loader import DataLoader\n",
    "from src.ner.evaluate import evaluate\n",
    "from src.booster.progressive_encoder import CharEncoder, EntityEncoder, WordEncoder\n",
    "from src.booster.progNN.net import LSTMCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root_path, 'src/ner/data/bc5cdr_iobes_id')\n",
    "model_dir = os.path.join(root_path, 'src/ner/experiments/test/test_transfer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_dir = os.path.join(root_path, 'src/ner/experiments/disease/st/st_ncbi')\n",
    "all_layer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set the device to train on\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the parameters from json file\n",
    "network_params = os.path.join(pretrained_model_dir, 'params.json') # these should be loaded from the pretrained model\n",
    "new_network_params = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(network_params), \"No json configuration file found at {}\".format(network_params)\n",
    "assert os.path.isfile(new_network_params), \"No json configuration file found at {}\".format(new_network_params)\n",
    "params = utils.Params(network_params)\n",
    "new_network_params = utils.Params(new_network_params)\n",
    "params.cuda = torch.cuda.is_available() # use GPU if available\n",
    "new_network_params.cuda = torch.cuda.is_available() # use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "- done.\n",
      "train size: 4559\n",
      "val size: 4580\n",
      "test size: 4796\n"
     ]
    }
   ],
   "source": [
    "# 5. Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "from collections import OrderedDict\n",
    "data_encoder = utils.load_obj(os.path.join(pretrained_model_dir, 'data_encoder.pkl'))\n",
    "pretrained_label_encoder = utils.load_obj(os.path.join(pretrained_model_dir, 'label_encoder.pkl'))\n",
    "label_encoder = OrderedDict()\n",
    "label_encoder[EntityEncoder.FEATURE_NAME] = EntityEncoder(os.path.join(data_dir, 'feats'))\n",
    "\n",
    "# 5.2 load data\n",
    "data_loader = DataLoader(params, data_dir, data_encoder, label_encoder)\n",
    "data = data_loader.load_data(['train', 'val', 'test'], data_dir)\n",
    "train_data = data['train']\n",
    "val_data = data['val']\n",
    "test_data = data['test']\n",
    "\n",
    "# 5.3 specify the train and val dataset sizes\n",
    "new_network_params.train_size = train_data['size']\n",
    "new_network_params.val_size = val_data['size']\n",
    "new_network_params.test_size = test_data['size']\n",
    "logging.info(\"- done.\")\n",
    "logging.info('train size: {}'.format(new_network_params.train_size))\n",
    "logging.info('val size: {}'.format(new_network_params.val_size))\n",
    "logging.info('test size: {}'.format(new_network_params.test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pretrained model: ../../../src/ner/experiments/disease/st/st_ncbi\n",
      "all layer: False\n",
      "new model : ../../../src/ner/experiments/test/test_transfer\n",
      "new data: ../../../src/ner/data/bc5cdr_iobes_id\n",
      "C:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params:  40434058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading pretrained model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params:  2130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loaded pretrained model\n"
     ]
    }
   ],
   "source": [
    "# 6. Modeling\n",
    "logging.info('pretrained model: {}'.format(pretrained_model_dir))\n",
    "logging.info('all layer: {}'.format(all_layer))\n",
    "logging.info('new model : {}'.format(model_dir))\n",
    "logging.info('new data: {}'.format(data_dir))\n",
    "# 6.1 Define the model\n",
    "model = LSTMCRF(params=params,\n",
    "                char_vocab_length=data_encoder[CharEncoder.FEATURE_NAME].vocab_length,\n",
    "                num_tags=pretrained_label_encoder[EntityEncoder.FEATURE_NAME].num_tags,\n",
    "                pretrained_word_vecs=torch.from_numpy(data_encoder[WordEncoder.FEATURE_NAME].vectors),\n",
    "                dropout=params.dropout,\n",
    "                decoder_type=params.decoder,\n",
    "                bidirectional=params.rnn_bidirectional,\n",
    "                freeze_embeddings=params.freeze_wordembeddings).to(device).float()\n",
    "model_total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print('total params: ', model_total_params)\n",
    "# load the pre-trained model\n",
    "logging.info('loading pretrained model')\n",
    "utils.load_checkpoint(os.path.join(pretrained_model_dir, 'best.pth'), model)\n",
    "if not all_layer:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "model.reset_layers(label_encoder[EntityEncoder.FEATURE_NAME].num_tags)\n",
    "model.to(device).float()\n",
    "\n",
    "model_total_trainable_params = sum(p.numel() for p in filter(lambda p: p.requires_grad,\n",
    "                                                             model.parameters()))\n",
    "print('total trainable params: ', model_total_trainable_params)\n",
    "\n",
    "logging.info('loaded pretrained model')\n",
    "optimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                      lr=new_network_params.learning_rate,\n",
    "                      momentum=params.momentum)\n",
    "\n",
    "# 6.2 fetch loss function and metrics\n",
    "from src.ner.evaluation import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "metrics = {'accuracy': accuracy_score,\n",
    "           'f1_score': f1_score,  # micro F1 score\n",
    "           'precision_score': precision_score,\n",
    "           'recall_score': recall_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting training for 200 epoch(s)\n",
      "Epoch 1/2\n",
      "Learning Rate : [0.015]\n",
      "100%|███████████████████████████████████████████████████████████████████| 456/456 [00:25<00:00, 17.62it/s, loss=67.701]\n",
      "- Train metrics: accuracy: 0.693 ; f1_score: 0.218 ; precision_score: 0.417 ; recall_score: 0.147 ; loss: 198.494\n",
      "- Val Eval metrics : accuracy: 0.915 ; f1_score: 0.472 ; precision_score: 0.359 ; recall_score: 0.690 ; loss: 44.932\n",
      "- Test Eval metrics : accuracy: 0.915 ; f1_score: 0.455 ; precision_score: 0.348 ; recall_score: 0.659 ; loss: 45.215\n",
      "- Found new best F1 score\n",
      "Epoch 2/2\n",
      "Learning Rate : [0.014285714285714284]\n",
      "100%|███████████████████████████████████████████████████████████████████| 456/456 [00:26<00:00, 17.47it/s, loss=54.570]\n",
      "- Train metrics: accuracy: 0.892 ; f1_score: 0.505 ; precision_score: 0.435 ; recall_score: 0.602 ; loss: 62.567\n",
      "- Val Eval metrics : accuracy: 0.915 ; f1_score: 0.462 ; precision_score: 0.349 ; recall_score: 0.681 ; loss: 43.462\n",
      "- Test Eval metrics : accuracy: 0.917 ; f1_score: 0.450 ; precision_score: 0.342 ; recall_score: 0.660 ; loss: 43.827\n",
      "current patience: 1 ; max patience: 20\n"
     ]
    }
   ],
   "source": [
    "# 7. Train the model\n",
    "\"\"\"\n",
    "The evaluation is done on train, val, and test set, but the model is saved based on the val set. This is\n",
    "to avoid extra overhead of running the evaluate.py script.\n",
    "params: restore_file: specify the model path to finetune\n",
    "params: save_model: Boolean, to save model at the end of the epoch or not. Model is saved for the best F1 score on validation set\n",
    "params: eval: whether to run the evaluation on the validation set or not.\n",
    "\"\"\"\n",
    "from src.booster.algorithms.fine_tune import train_and_evaluate\n",
    "# 7. Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "end_epoch = train_and_evaluate(model=model,\n",
    "                               data_loader=data_loader,\n",
    "                               train_data=train_data,\n",
    "                               val_data=val_data,\n",
    "                               test_data=test_data,\n",
    "                               optimizer=optimizer,\n",
    "                               metrics=metrics,\n",
    "                               params=new_network_params,\n",
    "                               model_dir=model_dir,\n",
    "                               data_encoder=data_encoder,\n",
    "                               label_encoder=label_encoder,\n",
    "                               restore_file=None,\n",
    "                               save_model=False,\n",
    "                               eval=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
