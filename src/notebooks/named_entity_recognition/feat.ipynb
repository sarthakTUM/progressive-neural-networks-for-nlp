{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions to run:\n",
    "\n",
    "1. Specify the data folder. Currently it assumes that the data folder contains train, test, and val folders.\n",
    "2. Specify the word embeddings to be used. Currently, it assumes that the word embeddings are stored in the src/ner/embeddings folder, as a txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root_path = '../../../'\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "from src.booster.progressive_encoder import WordEncoder, CharEncoder, EntityEncoder\n",
    "from src.ner.data import SLIterator\n",
    "from src.ner import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_to_txt_file(vocab, txt_path):\n",
    "    \"\"\"Writes one token per line, 0-based line id corresponds to the id of the token.\n",
    "\n",
    "    Args:\n",
    "        vocab: (iterable object) yields token\n",
    "        txt_path: (stirng) path to vocab file\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for token in vocab:\n",
    "            f.write(token + '\\n')\n",
    "            \n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict to json file\n",
    "\n",
    "    Args:\n",
    "        d: (dict)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        d = {k: v for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)\n",
    "\n",
    "\n",
    "def update_vocab(txt_path, vocab):\n",
    "    \"\"\"Update word and tag vocabulary from dataset\n",
    "\n",
    "    Args:\n",
    "        txt_path: (string) path to file, one sentence per line\n",
    "        vocab: (dict or Counter) with update method\n",
    "\n",
    "    Returns:\n",
    "        dataset_size: (int) number of elements in the dataset\n",
    "    \"\"\"\n",
    "    with open(txt_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            vocab.update(line.strip().split(' '))\n",
    "\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "def get_stats(data_iterator, words, tags):\n",
    "    sentence_gen = data_iterator.get_next_X()\n",
    "    label_gen = data_iterator.get_next_Y()\n",
    "    num_sentences = 0\n",
    "    num_labels = 0\n",
    "    for sent, label in zip(sentence_gen,label_gen):\n",
    "        assert len(sent) == len(label)\n",
    "        words.update(sent)\n",
    "        tags.update(label)\n",
    "        if sent:\n",
    "            num_sentences += 1\n",
    "        if label:\n",
    "            num_labels += 1\n",
    "    assert num_sentences == num_labels\n",
    "    return num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the data directory, containing train, val, and test folders.\n",
    "\n",
    "min_word_count = 1\n",
    "min_count_tag=1\n",
    "data_folder = os.path.join(root_path, 'src/ner/data/bc5cdr_iobes_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train\n",
      "loading val\n",
      "loading test\n"
     ]
    }
   ],
   "source": [
    "# load the datasets\n",
    "\n",
    "print('loading train')\n",
    "train_data_iterator = SLIterator(os.path.join(data_folder, 'train'))\n",
    "print('loading val')\n",
    "val_data_iterator = SLIterator(os.path.join(data_folder, 'val'))\n",
    "print('loading test')\n",
    "test_data_iterator = SLIterator(os.path.join(data_folder, 'test'))\n",
    "data_iterators = [train_data_iterator, val_data_iterator, test_data_iterator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting stats\n"
     ]
    }
   ],
   "source": [
    "# get the stats\n",
    "\n",
    "words = Counter()\n",
    "tags = Counter()\n",
    "print('getting stats')\n",
    "num_sentences_train = get_stats(train_data_iterator, words, tags)\n",
    "num_sentences_val = get_stats(val_data_iterator, words, tags)\n",
    "num_sentences_test = get_stats(test_data_iterator, words, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding\n"
     ]
    }
   ],
   "source": [
    "# for all types of features for which vocab has to be created, load encoders\n",
    "print('encoding')\n",
    "from collections import OrderedDict\n",
    "data_encoders = OrderedDict()\n",
    "label_encoders = OrderedDict()\n",
    "data_encoders[WordEncoder.FEATURE_NAME] = WordEncoder(os.path.join(root_path,'src/ner/embeddings/glove.6B.100d.txt'), \n",
    "                                                      dim=100)\n",
    "data_encoders[CharEncoder.FEATURE_NAME] = CharEncoder()\n",
    "label_encoders[EntityEncoder.FEATURE_NAME] = EntityEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating and saving maps..\n",
      "creating map: WORD\n",
      "creating map: CHAR\n",
      "Char distribution:  Counter({'e': 188548, 'i': 135258, 't': 132076, 'a': 130865, 'n': 119724, 'o': 113597, 'r': 100813, 's': 95445, 'd': 67059, 'c': 63740, 'l': 61000, 'h': 52613, 'p': 41964, 'm': 41760, 'u': 40683, 'f': 33563, 'y': 27208, 'g': 26253, 'w': 18493, '.': 18226, 'v': 18100, 'b': 17712, ',': 12223, '-': 10548, 'T': 6583, '0': 6422, ')': 6068, '(': 6033, '1': 5812, 'x': 5671, 'A': 5281, 'k': 4908, 'C': 4472, '2': 4397, 'S': 4390, 'I': 3863, 'P': 3654, '5': 3498, 'D': 3059, 'N': 3034, '3': 2895, 'E': 2868, '/': 2866, 'z': 2737, 'O': 2706, 'R': 2500, '4': 2499, 'M': 2318, 'H': 2212, 'L': 2087, ':': 1939, '6': 1871, 'B': 1868, '%': 1709, '7': 1572, '8': 1564, '9': 1522, 'U': 1400, 'F': 1381, 'G': 1363, 'j': 1282, 'q': 1061, 'W': 995, 'V': 922, '+': 890, '=': 683, ';': 662, 'K': 591, \"'\": 436, '<': 370, 'J': 259, 'X': 254, 'Y': 191, ']': 188, '[': 187, 'Z': 185, 'Q': 165, '\"': 147, '>': 131, '?': 30, '_': 4, '~': 2, '__PAD__': 1, '__UNK__': 1, '{': 1, '}': 1, '&': 1})\n",
      "creating map: ENTITY\n",
      "Entity distribution:  Counter({'O': 317854, 'S-Chemical': 13786, 'S-Disease': 7585, 'B-Disease': 5263, 'E-Disease': 5263, 'I-Chemical': 3130, 'I-Disease': 3110, 'B-Chemical': 2144, 'E-Chemical': 2144, '__PAD__': 1})\n"
     ]
    }
   ],
   "source": [
    "# encode the dataset\n",
    "\n",
    "print('creating and saving maps..')\n",
    "feats_folder = os.path.join(data_folder, 'feats')\n",
    "if not os.path.exists(feats_folder):\n",
    "    os.makedirs(feats_folder)\n",
    "    \n",
    "# create vocabs with iterators and encoders.\n",
    "for feature_name, encoder in data_encoders.items():\n",
    "    print('creating map: {}'.format(feature_name))\n",
    "    encoder.create_map(data_iterators)\n",
    "    # save maps in the data folder.\n",
    "    for map_name, map in encoder.maps.items():\n",
    "        utils.save_map(map, map_name, feats_folder)\n",
    "\n",
    "for feature_name, encoder in label_encoders.items():\n",
    "    print('creating map: {}'.format(feature_name))\n",
    "    encoder.create_map(data_iterators)\n",
    "    # save maps in the data folder.\n",
    "    for map_name, map in encoder.maps.items():\n",
    "        utils.save_map(map, map_name, feats_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vocabularies to file...\n",
      "- done.\n",
      "saving dataset stats\n",
      "Characteristics of the dataset:\n",
      "- train_size: 4559\n",
      "- val_size: 4580\n",
      "- test_size: 4796\n",
      "- vocab_size: 17379\n",
      "- number_of_tags: 9\n",
      "- special_tokens: {'pad_WORD': '__PAD__', 'unk_WORD': '__UNK__', 'pad_CHAR': '__PAD__', 'unk_CHAR': '__UNK__', 'pad_ENTITY': '__PAD__', 'unk_ENTITY': None}\n",
      "- data_iterators: {'train': 'SLIterator', 'val': 'SLIterator', 'test': 'SLIterator'}\n"
     ]
    }
   ],
   "source": [
    "# Save vocabularies to file\n",
    "print(\"Saving vocabularies to file...\")\n",
    "save_vocab_to_txt_file(words, os.path.join(data_folder, 'words.txt'))\n",
    "save_vocab_to_txt_file(tags, os.path.join(data_folder, 'tags.txt'))\n",
    "print(\"- done.\")\n",
    "\n",
    "# Save datasets properties in json file\n",
    "print('saving dataset stats')\n",
    "encoder_params = dict()\n",
    "encoders = [data_encoders, label_encoders]\n",
    "for encoder in encoders:\n",
    "    for encoder_name, enc in encoder.items():\n",
    "        encoder_params['pad_'+encoder_name] = enc.PAD\n",
    "        encoder_params['unk_'+encoder_name] = enc.UNK\n",
    "\n",
    "sizes = {\n",
    "    'train_size': num_sentences_train,\n",
    "    'val_size': num_sentences_val,\n",
    "    'test_size': num_sentences_test,\n",
    "    'vocab_size': len(words),\n",
    "    'number_of_tags': len(tags),\n",
    "    'special_tokens': encoder_params,\n",
    "    'data_iterators': {'train': train_data_iterator.__class__.__name__,\n",
    "                       'val': val_data_iterator.__class__.__name__,\n",
    "                       'test': test_data_iterator.__class__.__name__}\n",
    "}\n",
    "save_dict_to_json(sizes, os.path.join(data_folder, 'dataset_params.json'))\n",
    "\n",
    "# Logging sizes\n",
    "to_print = \"\\n\".join(\"- {}: {}\".format(k, v) for k, v in sizes.items())\n",
    "print(\"Characteristics of the dataset:\\n{}\".format(to_print))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
