{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root_path = '../../../'\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from src.tc import utils\n",
    "from collections import OrderedDict\n",
    "from src.ner.encoder import CharEncoder, ClassEncoder, WordEncoder\n",
    "from src.tc.model.net import CNNTC\n",
    "from src.tc.data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root_path, 'src/tc/data/sst_binary')\n",
    "model_dir = os.path.join(root_path, 'src/tc/experiments/sst_binary_testjupyter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set the device to train on\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the parameters from json file\n",
    "network_params = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(network_params), \"No json configuration file found at {}\".format(network_params)\n",
    "params = utils.Params(network_params)\n",
    "params.cuda = torch.cuda.is_available() # use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading encoders\n"
     ]
    }
   ],
   "source": [
    "# 5. specify features\n",
    "logging.info('loading encoders')\n",
    "data_encoder = OrderedDict()\n",
    "label_encoder = OrderedDict()\n",
    "data_encoder[CharEncoder.FEATURE_NAME] = CharEncoder(os.path.join(data_dir, 'feats'))\n",
    "data_encoder[WordEncoder.FEATURE_NAME] = WordEncoder(os.path.join(data_dir, 'feats'))\n",
    "# data_encoder[WordEncoder.FEATURE_NAME] = WordEncoder(os.path.join('data/sst_binary', 'feats_w2v'))\n",
    "label_encoder[ClassEncoder.FEATURE_NAME] = ClassEncoder(os.path.join(data_dir, 'feats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for: ../../../src/tc/data/sst_binary\n",
      "- done.\n",
      "train size: 83881\n",
      "val size: 872\n",
      "test size: 1821\n",
      "Starting training for 1000 epoch(s)\n",
      "Epoch 1/1000\n",
      "100%|██████████████████████████████████████████████████████████████████| 1677/1677 [00:25<00:00, 65.31it/s, loss=0.401]\n",
      "- Train metrics: accuracy: 0.808 ; loss: 0.401\n",
      "- Eval metrics : accuracy: 0.806 ; loss: 0.449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 2/1000\n",
      "100%|██████████████████████████████████████████████████████████████████| 1677/1677 [00:35<00:00, 47.68it/s, loss=0.310]\n",
      "- Train metrics: accuracy: 0.857 ; loss: 0.310\n",
      "- Eval metrics : accuracy: 0.814 ; loss: 0.438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e10ca02f907b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m                                    \u001b[0mrestore_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                                    \u001b[0msave_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                                    eval=True)\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m# 8. Train on dev set if required for n_epochs where n_epochs is returned from training on train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Thesis\\thesis-sarthak\\src\\tc\\train.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_data, val_data, data_loader, optimizer, metrics, params, model_dir, data_encoder, label_encoder, restore_file, save_model, eval)\u001b[0m\n\u001b[0;32m    189\u001b[0m                                    'optim_dict': optimizer.state_dict()},\n\u001b[0;32m    190\u001b[0m                                   \u001b[0mis_best\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_best\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                                   checkpoint=model_dir)\n\u001b[0m\u001b[0;32m    192\u001b[0m             \u001b[1;31m# save encoders only if they do not exist yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data_encoder.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Thesis\\thesis-sarthak\\src\\ner\\utils.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[1;34m(state, is_best, checkpoint)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Checkpoint Directory exists! \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_best\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'best.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[1;34m(f, mode, body)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[0mserialized_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_is_real_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k_fold = None\n",
    "combine_train_dev = False\n",
    "train_on_dev = False\n",
    "\n",
    "# create a data loader\n",
    "data_loader = DataLoader(params, data_dir, data_encoder, label_encoder)\n",
    "if k_fold:\n",
    "    # each split will have a separate directory\n",
    "    logging.info('K-Fold turned on with folds: {}'.format(k_fold))\n",
    "    splits_dir = [os.path.join(data_dir, 'split_'+str(split_num)) for split_num in range(1, k_fold+1)]\n",
    "else:\n",
    "    splits_dir = [data_dir]\n",
    "\n",
    "for split_dir in splits_dir:\n",
    "    logging.info('training for: {}'.format(split_dir))\n",
    "    data_dir = split_dir\n",
    "    if k_fold:\n",
    "        # for each split, make a new model directory for that particular split\n",
    "        split_model_dir = os.path.join(model_dir, os.path.basename(split_dir))\n",
    "        if not os.path.exists(split_model_dir):\n",
    "            os.makedirs(split_model_dir)\n",
    "    else:\n",
    "        split_model_dir = model_dir\n",
    "        \n",
    "    # load the respective datasets\n",
    "    data = data_loader.load_data(['train', 'val', 'test'], data_dir)\n",
    "    train_data = data['train']\n",
    "    val_data = data['val']\n",
    "    test_data = data['test']\n",
    "\n",
    "    # combine train and val data\n",
    "    if combine_train_dev:\n",
    "        logging.info('combining train and dev sets')\n",
    "        for k, v in train_data.items():\n",
    "            if isinstance(train_data[k], list):\n",
    "                train_data[k].extend(val_data[k])\n",
    "            elif isinstance(train_data[k], int):\n",
    "                train_data[k] += val_data[k]\n",
    "\n",
    "    # 5.3 specify the train and val dataset sizes\n",
    "    params.train_size = train_data['size']\n",
    "    params.val_size = val_data['size']\n",
    "    params.test_size = test_data['size']\n",
    "    logging.info(\"- done.\")\n",
    "    logging.info('train size: {}'.format(params.train_size))\n",
    "    logging.info('val size: {}'.format(params.val_size))\n",
    "    logging.info('test size: {}'.format(params.test_size))\n",
    "    \n",
    "    # 6. Modeling\n",
    "    # 6.1 Define the model and the optimizer\n",
    "    model = CNNTC(params=params,\n",
    "                  char_vocab_length=data_encoder[CharEncoder.FEATURE_NAME].vocab_length,\n",
    "                  num_tags=label_encoder[ClassEncoder.FEATURE_NAME].num_tags,\n",
    "                  pretrained_word_vecs=torch.from_numpy(data_encoder[WordEncoder.FEATURE_NAME].vectors),\n",
    "                  dropout=params.dropout,\n",
    "                  decoder_type=params.decoder,\n",
    "                  bidirectional=params.rnn_bidirectional,\n",
    "                  freeze_embeddings=params.freeze_wordembeddings).to(device).float()\n",
    "    optimizer = optim.Adadelta(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                               rho=0.95)\n",
    "\n",
    "    # 6.2 define metrics\n",
    "    from src.ner.evaluation import accuracy_score\n",
    "    metrics = {'accuracy': accuracy_score}\n",
    "\n",
    "    # 7. Train the model\n",
    "    from src.tc.train import train_and_evaluate\n",
    "    logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "    end_epoch = train_and_evaluate(model=model,\n",
    "                                   train_data=train_data,\n",
    "                                   val_data=test_data if combine_train_dev else val_data,\n",
    "                                   data_loader=data_loader,\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=metrics,\n",
    "                                   params=params,\n",
    "                                   model_dir=split_model_dir,\n",
    "                                   data_encoder=data_encoder,\n",
    "                                   label_encoder=label_encoder,\n",
    "                                   restore_file=None,\n",
    "                                   save_model='val',\n",
    "                                   eval=True)\n",
    "    \n",
    "    # 8. Train on dev set if required for n_epochs where n_epochs is returned from training on train set\n",
    "    if train_on_dev:\n",
    "        logging.info('training on train and dev for {} epochs'.format(end_epoch+1))\n",
    "        # 8.1. combine train and dev datasets\n",
    "        data = data_loader.load_data(['train', 'val', 'test'], data_dir)\n",
    "        train_data = data['train']\n",
    "        val_data = data['val']\n",
    "        test_data = data['test']\n",
    "        logging.info('combining train and dev sets')\n",
    "        for k, v in train_data.items():\n",
    "            if isinstance(train_data[k], list):\n",
    "                train_data[k].extend(val_data[k])\n",
    "            elif isinstance(train_data[k], int):\n",
    "                train_data[k] += val_data[k]\n",
    "        params.train_size = train_data['size']\n",
    "        params.val_size = test_data['size']\n",
    "        logging.info(\"- done.\")\n",
    "        logging.info('train size: {}'.format(params.train_size))\n",
    "        logging.info('test size: {}'.format(params.val_size))\n",
    "\n",
    "        # 8.2. delete old model and optimizer\n",
    "        del model\n",
    "        del optimizer\n",
    "\n",
    "        # 8.3. construct a new model and optimizer\n",
    "        model = CNNTC(params=params,\n",
    "                      char_vocab_length=data_encoder[CharEncoder.FEATURE_NAME].vocab_length,\n",
    "                      num_tags=label_encoder[ClassEncoder.FEATURE_NAME].num_tags,\n",
    "                      pretrained_word_vecs=torch.from_numpy(data_encoder[WordEncoder.FEATURE_NAME].vectors),\n",
    "                      dropout=params.dropout,\n",
    "                      decoder_type=params.decoder,\n",
    "                      bidirectional=params.rnn_bidirectional,\n",
    "                      freeze_embeddings=params.freeze_wordembeddings).to(device).float()\n",
    "        optimizer = optim.Adadelta(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                   rho=0.95)\n",
    "\n",
    "        # 8.4. train without evaluating with new number of epochs\n",
    "        params.num_epochs = end_epoch+1\n",
    "        final_model_dir = os.path.join(split_model_dir, 'final')\n",
    "        if not os.path.exists(final_model_dir):\n",
    "            os.makedirs(final_model_dir)\n",
    "        train_and_evaluate(model=model,\n",
    "                           train_data=train_data,\n",
    "                           val_data=test_data,\n",
    "                           data_loader=data_loader,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=metrics,\n",
    "                           params=params,\n",
    "                           model_dir=final_model_dir,\n",
    "                           data_encoder=data_encoder,\n",
    "                           label_encoder=label_encoder,\n",
    "                           restore_file=None,\n",
    "                           save_model='train',\n",
    "                           eval=True)\n",
    "    del model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
