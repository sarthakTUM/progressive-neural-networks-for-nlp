{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a network for Named Entity Recognition with architecture mentioned in [].\n",
    "\n",
    "How to run:\n",
    "1. Specify the data_dir. The directory should contain the train, val, and test folders, along with the 'feats' folder obtained through feat.ipynb.\n",
    "2. specify the model directory. The directory needs to be created manually. It should contain a params.json.\n",
    "3. The description of the progNet specific parameters is given below in the respective cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root_path = '../../../'\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm import trange\n",
    "from src.ner import utils\n",
    "from src.booster.progressive_data_loader import DataLoader\n",
    "from src.booster.progressive_evaluate import evaluate\n",
    "from src.booster.progressive_encoder import CharEncoder, EntityEncoder, WordEncoder\n",
    "from src.booster.progNN.net import LSTMCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root_path, 'src/ner/data/bc5cdr_iobes_id')\n",
    "model_dir = os.path.join(root_path, 'src/ner/experiments/test/test_prog3col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "freeze_prev: whether to freeze the previous column\n",
    "best_prev: whether to load the best checkpoint of the previous column, or a randomly initialized network. This is to see whether there is some benefit from the PNN structure, or just through more capacity.\n",
    "linear_adapter: keep it to True mostly. False places a non-linear adapter. Check ProgNet paper for more details.\n",
    "\n",
    "The parameters below are optimal.\n",
    "\"\"\"\n",
    "\n",
    "freeze_prev = True\n",
    "best_prev = True\n",
    "linear_adapter = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "c1_model_dir: model directory for the first column\n",
    "c2_model_dir: model directory for the second column\n",
    "\"\"\"\n",
    "\n",
    "# 0. pretrained model dir\n",
    "c1_model_dir = os.path.join(root_path, 'src/ner/experiments/disease/st/st_ncbi')\n",
    "c2_model_dir = os.path.join(root_path, 'src/ner/experiments/disease/st/st_jnlpba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set the device to train on\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the parameters from json file\n",
    "c1_network_params = os.path.join(c1_model_dir, 'params.json') # these should be loaded from the pretrained model\n",
    "assert os.path.isfile(c1_network_params), \"No json configuration file found at {}\".format(c1_network_params)\n",
    "\n",
    "c2_network_params = os.path.join(c2_model_dir, 'params.json') # these should be loaded from the pretrained model\n",
    "assert os.path.isfile(c2_network_params), \"No json configuration file found at {}\".format(c2_network_params)\n",
    "\n",
    "new_network_params = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(new_network_params), \"No json configuration file found at {}\".format(new_network_params)\n",
    "\n",
    "c1_params = utils.Params(c1_network_params)\n",
    "c2_params = utils.Params(c2_network_params)\n",
    "new_params = utils.Params(new_network_params)\n",
    "\n",
    "# use GPU if available\n",
    "c1_params.cuda = torch.cuda.is_available()\n",
    "c2_params.cuda = torch.cuda.is_available()\n",
    "new_params.cuda = torch.cuda.is_available()\n",
    "\n",
    "# 3. Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if new_params.cuda: torch.cuda.manual_seed(230)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "creating and loading data loaders\n"
     ]
    }
   ],
   "source": [
    "# 5. Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "# 5.1 specify features\n",
    "from collections import OrderedDict\n",
    "\n",
    "# 5.1.1 encoders for the new model\n",
    "logging.info('creating and loading data loaders')\n",
    "data_encoder = OrderedDict()\n",
    "data_encoder[CharEncoder.FEATURE_NAME] = CharEncoder(os.path.join(data_dir, 'feats'))\n",
    "data_encoder[WordEncoder.FEATURE_NAME] = WordEncoder(os.path.join(data_dir, 'feats'), dim=new_params.embedding_dim)\n",
    "label_encoder = OrderedDict()\n",
    "label_encoder[EntityEncoder.FEATURE_NAME] = EntityEncoder(os.path.join(data_dir, 'feats'))\n",
    "new_params.data_feats = []\n",
    "new_params.label_feats = []\n",
    "for feat in data_encoder:\n",
    "    new_params.data_feats.append(feat)\n",
    "for feat in label_encoder:\n",
    "    new_params.label_feats.append(feat)\n",
    "\n",
    "# 5.1.2 encoders for the 1st model\n",
    "c1_data_encoder = utils.load_obj(os.path.join(c1_model_dir, 'data_encoder.pkl'))\n",
    "c1_label_encoder = utils.load_obj(os.path.join(c1_model_dir, 'label_encoder.pkl'))\n",
    "c1_params.data_feats = []\n",
    "c1_params.label_feats = []\n",
    "for feat in c1_data_encoder:\n",
    "    c1_params.data_feats.append(feat)\n",
    "for feat in c1_label_encoder:\n",
    "    c1_params.label_feats.append(feat)\n",
    "\n",
    "# 5.1.3 encoders for the 2nd model\n",
    "c2_data_encoder = utils.load_obj(os.path.join(c2_model_dir, 'data_encoder.pkl'))\n",
    "c2_label_encoder = utils.load_obj(os.path.join(c2_model_dir, 'label_encoder.pkl'))\n",
    "c2_params.data_feats = []\n",
    "c2_params.label_feats = []\n",
    "for feat in c2_data_encoder:\n",
    "    c2_params.data_feats.append(feat)\n",
    "for feat in c2_label_encoder:\n",
    "    c2_params.label_feats.append(feat)\n",
    "\n",
    "# 5.2 load data\n",
    "# 5.2.1 data loader for the new model\n",
    "data_loader = DataLoader(new_params,\n",
    "                         data_dir,\n",
    "                         data_encoder,\n",
    "                         label_encoder,\n",
    "                         device=device)\n",
    "c1_data_loader = DataLoader(c1_params,\n",
    "                            data_dir,# data_dir has to be the new data\n",
    "                            c1_data_encoder,\n",
    "                            device=device)\n",
    "\n",
    "c2_data_loader = DataLoader(c2_params,\n",
    "                            data_dir,  # data_dir has to be the new data\n",
    "                            c2_data_encoder,\n",
    "                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "freeze_prev: True\n",
      "best_prev: True\n",
      "loading previous models and creating new model\n",
      "C:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# 6. Modeling\n",
    "logging.info('freeze_prev: {}'.format(str(freeze_prev)))\n",
    "logging.info('best_prev: {}'.format(str(best_prev)))\n",
    "# 6.1.1 Define the model architecture for 1st and 2nd column\n",
    "logging.info('loading previous models and creating new model')\n",
    "c1_model = LSTMCRF(params=c1_params,\n",
    "                   char_vocab_length=c1_data_encoder[CharEncoder.FEATURE_NAME].vocab_length,\n",
    "                   num_tags=c1_label_encoder[EntityEncoder.FEATURE_NAME].num_tags,\n",
    "                   pretrained_word_vecs=torch.from_numpy(c1_data_encoder[WordEncoder.FEATURE_NAME].vectors),\n",
    "                   dropout=c1_params.dropout,\n",
    "                   decoder_type=c1_params.decoder,\n",
    "                   bidirectional=c1_params.rnn_bidirectional,\n",
    "                   freeze_embeddings=c1_params.freeze_wordembeddings).to(device).float()\n",
    "\n",
    "c2_model = LSTMCRF(params=c2_params,\n",
    "                   char_vocab_length=c2_data_encoder[CharEncoder.FEATURE_NAME].vocab_length,\n",
    "                   num_tags=c2_label_encoder[EntityEncoder.FEATURE_NAME].num_tags,\n",
    "                   pretrained_word_vecs=torch.from_numpy(c2_data_encoder[WordEncoder.FEATURE_NAME].vectors),\n",
    "                   dropout=c2_params.dropout,\n",
    "                   decoder_type=c2_params.decoder,\n",
    "                   bidirectional=c2_params.rnn_bidirectional,\n",
    "                   freeze_embeddings=c2_params.freeze_wordembeddings).to(device).float()\n",
    "# 6.1.1.1. load the pre-trained model to fit the architecture\n",
    "if best_prev:\n",
    "    utils.load_checkpoint(os.path.join(c1_model_dir, 'best.pth'), c1_model)\n",
    "    utils.load_checkpoint(os.path.join(c2_model_dir, 'best.pth'), c2_model)\n",
    "\n",
    "# 6.1.2 Define the model for target column\n",
    "c3_model = LSTMCRF(params=new_params,\n",
    "                   char_vocab_length=data_encoder[CharEncoder.FEATURE_NAME].vocab_length,\n",
    "                   num_tags=label_encoder[EntityEncoder.FEATURE_NAME].num_tags,\n",
    "                   pretrained_word_vecs=torch.from_numpy(data_encoder[WordEncoder.FEATURE_NAME].vectors),\n",
    "                   dropout=new_params.dropout,\n",
    "                   decoder_type=new_params.decoder,\n",
    "                   bidirectional=new_params.rnn_bidirectional,\n",
    "                   freeze_embeddings=new_params.freeze_wordembeddings).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating columns\n"
     ]
    }
   ],
   "source": [
    "# 7. Convert to columns\n",
    "\"\"\"\n",
    "the columns must specify the name of the layers to be used for PNN transfer, in the order from input to output.\n",
    "The numbers in the bracket represent the input and output dimension of the layer respectively.\n",
    "\"\"\"\n",
    "logging.info('creating columns')\n",
    "from src.booster.progNN.column import Column\n",
    "column_1 = Column(model=c1_model,\n",
    "                  layers={'rnn_1': (130, 200),\n",
    "                          'rnn_2': (200, 200),\n",
    "                          'fc':  (200, c1_label_encoder[EntityEncoder.FEATURE_NAME].num_tags)},\n",
    "                  data_loader=c1_data_loader).to(device)\n",
    "\n",
    "column_2 = Column(model=c2_model,\n",
    "                  layers={'rnn_1': (130, 200),\n",
    "                          'rnn_2': (200, 200),\n",
    "                          'fc':  (200, c2_label_encoder[EntityEncoder.FEATURE_NAME].num_tags)},\n",
    "                  data_loader=c2_data_loader).to(device)\n",
    "\n",
    "column_3 = Column(model=c3_model,\n",
    "                  layers={'rnn_1': (130, 200),\n",
    "                          'rnn_2': (200, 200),\n",
    "                          'fc':  (200, label_encoder[EntityEncoder.FEATURE_NAME].num_tags)},\n",
    "                  data_loader=data_loader).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "linear adapter: True\n",
      "creating progressive net\n",
      "total params: 121391698\n",
      "total trainable params: 519090\n"
     ]
    }
   ],
   "source": [
    "from src.booster.progNN.adapter import Adapter\n",
    "adapter = Adapter(prev_columns=[column_1, column_2], target_column=column_3, linear=linear_adapter).to(device)\n",
    "\n",
    "# 8. create progressive net\n",
    "logging.info('creating progressive net')\n",
    "from src.booster.progNN.prognet import ProgressiveNet\n",
    "\n",
    "# 8.1. load the progNet\n",
    "progNet = ProgressiveNet(prev_columns=[column_1, column_2],\n",
    "                         target_column=column_3,\n",
    "                         adapter=adapter,\n",
    "                         freeze_prev=freeze_prev).to(device).float()\n",
    "\n",
    "progNet_total_params = sum(p.numel() for p in progNet.parameters())\n",
    "progNet_total_trainable_params = sum(p.numel() for p in filter(lambda p: p.requires_grad,\n",
    "                                    progNet.parameters()))\n",
    "logging.info('total params: {}'.format(str(progNet_total_params)))\n",
    "logging.info('total trainable params: {}'.format(str(progNet_total_trainable_params)))\n",
    "\n",
    "# 8.2. define the metrics\n",
    "from src.ner.evaluation import accuracy_score, f1_score, precision_score, recall_score\n",
    "metrics = {'accuracy': accuracy_score,\n",
    "           'f1_score': f1_score,  # micro F1 score\n",
    "           'precision_score': precision_score,\n",
    "           'recall_score': recall_score}\n",
    "\n",
    "# 8.3. Define the optimizer\n",
    "optimizer = optim.SGD(params=filter(lambda p: p.requires_grad,\n",
    "                                    progNet.parameters()),\n",
    "                      lr=0.015,\n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progressive net\n",
      "Epoch 1/2\n",
      "Learning Rate : [0.015]\n",
      "100%|███████████████████████████████████████████████████████████████████| 456/456 [00:45<00:00, 10.13it/s, loss=67.675]\n",
      "- Train metrics: accuracy: 0.783 ; f1_score: 0.322 ; precision_score: 0.539 ; recall_score: 0.230 ; loss: 189.397\n",
      "- val metrics : accuracy: 0.933 ; f1_score: 0.666 ; precision_score: 0.611 ; recall_score: 0.732 ; loss: 38.025\n",
      "- test metrics : accuracy: 0.934 ; f1_score: 0.656 ; precision_score: 0.603 ; recall_score: 0.718 ; loss: 37.550\n",
      "- Found new best F1 score\n",
      "Epoch 2/2\n",
      "Learning Rate : [0.014285714285714284]\n",
      "100%|███████████████████████████████████████████████████████████████████| 456/456 [00:45<00:00,  9.98it/s, loss=49.787]\n",
      "- Train metrics: accuracy: 0.930 ; f1_score: 0.711 ; precision_score: 0.696 ; recall_score: 0.727 ; loss: 49.686\n",
      "- val metrics : accuracy: 0.934 ; f1_score: 0.694 ; precision_score: 0.673 ; recall_score: 0.717 ; loss: 33.989\n",
      "- test metrics : accuracy: 0.935 ; f1_score: 0.682 ; precision_score: 0.665 ; recall_score: 0.701 ; loss: 33.828\n",
      "- Found new best F1 score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Train ProgNet\n",
    "from src.booster.progressive_ner_3col import train_and_evaluate\n",
    "logging.info('Training progressive net')\n",
    "train_and_evaluate(progNet,\n",
    "                   progNet.target_column.data_loader,\n",
    "                   progNet.target_column.data['train'],\n",
    "                   progNet.target_column.data['val'],\n",
    "                   progNet.target_column.data['test'],\n",
    "                   optimizer,\n",
    "                   metrics,\n",
    "                   new_params,\n",
    "                   model_dir,\n",
    "                   progNet.target_column.data_loader.data_encoder,\n",
    "                   progNet.target_column.data_loader.label_encoder,\n",
    "                   restore_file=None,\n",
    "                   save_model=False,\n",
    "                   eval=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
